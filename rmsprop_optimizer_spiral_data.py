# -*- coding: utf-8 -*-
"""Training Simple Neural Network on Spiral Data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AZiP0izm-5XBWmvg4qPaQa1T3Yy2kRif

### <b>Dataset + Preprocessing</b>
"""

from neural_network_classes import Layer_Dense, Activation_ReLU, Activation_Softmax_Loss_CategoricalCrossEntropy, Optimizer_RMSProp

# pip install nnfs

from nnfs.datasets import spiral_data
import nnfs
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

nnfs.init()

"""Taking dataset with 3000 points and 3 classes - R, G and B with 1000 points belonging to each class."""

X, y = spiral_data(samples=1000, classes=3)

# X

# X.shape

# y

# y.shape

# plt.scatter(X[:,0], X[:,1], c=y, cmap='brg', s=6)
# plt.show()

"""Since the color map applied above is <b>brg</b>, it means that,<br>
<b>y = 0</b> means <b>Blue</b><br>
<b>y = 1</b> means <b>Red</b><br>
<b>y = 2</b> means <b>Green</b>

<b>Splitting into training and test data</b>
"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# X_train.shape

# y_train.shape

# X_test.shape

# y_test.shape

"""<b>There are no missing values in this dataset as it has been generated by us.</b>"""

# X_train.mean()

# X_test.mean()

# np.unique(y_train)

# np.unique(y_test)

"""<b>So, the dataset is already preprocessed.</b>

### <b>Training the model</b>
"""

def accuracy(y_pred, y_true):
  predictions = np.argmax(y_pred, axis=0)
  if (len(y_true.shape)==2):
    actual = np.argmax(y_true, axis=0)
  elif (len(y_true.shape)==1):
    actual = y_true
  acc = np.mean(predictions==actual) * 100
  return acc

"""<b>Initializing the optimizer</b>"""

optimizer = Optimizer_RMSProp(learning_rate=0.02, decay=1e-5, rho=0.999)

"""<b>Initializing all layers</b>"""

# Layer-1
dense1 = Layer_Dense(2, 64)
activation1 = Activation_ReLU()

# Layer-2
dense2 = Layer_Dense(64, 3)
loss_activation = Activation_Softmax_Loss_CategoricalCrossEntropy()

"""<b>Training the model</b>"""

n_epochs = 10000 # no. of iterations of complete forward and backward passes

for epoch in range(n_epochs):
  # Forward pass
  dense1.forward(X_train)
  activation1.forward(dense1.output)
  dense2.forward(activation1.output.T)
  net_loss = loss_activation.forward(dense2.output, y_train)
  acc = accuracy(loss_activation.output, y_train)
  if (epoch==0):
    initial_loss = net_loss
    initial_acc = acc
  print(f"Epoch-{epoch} - LR = {optimizer.current_learning_rate}, Loss = {net_loss}, Accuracy = {acc}")

  # Backward pass
  loss_activation.backward(loss_activation.output, y_train)
  dense2.backward(loss_activation.dinputs)
  # dense2.weights -= learning_rate * dense2.dweights
  # dense2.biases -= learning_rate * dense2.dbiases
  activation1.backward(dense2.dinputs.T)
  dense1.backward(activation1.dinputs)
  # dense1.weights -= learning_rate * dense1.dweights
  # dense1.biases -= learning_rate * dense1.dbiases

  # Updating the parameters
  optimizer.pre_update_params()
  optimizer.update_params(dense2)
  optimizer.update_params(dense1)
  optimizer.post_update_params()

"""### <b>Testing the model</b>"""

print(f"\nInitial Loss = {initial_loss}\nInitial Accuracy = {initial_acc}")

def test_model():
  dense1.forward(X_test)
  activation1.forward(dense1.output)
  dense2.forward(activation1.output.T)
  net_loss = loss_activation.forward(dense2.output, y_test)
  acc = accuracy(loss_activation.output, y_test)
  print(f"\nTesting the model\nLoss = {net_loss}, Accuracy = {acc}")

test_model()