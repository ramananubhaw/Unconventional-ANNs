# -*- coding: utf-8 -*-
"""Predicting Bet Outcome from Sports Betting Dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jJebAXgV6-tER35fg66b2IYaliidWuWp
"""

from neural_network_classes import Layer_Dense, Layer_Dropout, Activation_ReLU, Activation_Softmax_Loss_CategoricalCrossEntropy, Optimizer_Adam

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.metrics import accuracy_score

df = pd.read_csv('bets.csv', sep=';')

# df

df.drop('bet_id', axis=1, inplace=True)

# df

# df.isnull().sum()

# len(df['user_id'].unique())

# df['bet_type'].unique()

# df['sport'].unique()

# df['is_win'].unique()

# df['gain'].mean()

# df['gain'].min()

# df['gain'].max()

"""<b>Splitting into input features and output label</b>"""

X = df.drop(['user_id', 'gain', 'GGR', 'is_win'], axis=1)

# X

y = df['is_win']

# y

"""<b>Splitting into training and test data</b>"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# X_train.shape

# y_train.shape

# X_test.shape

# y_test.shape

"""<b>Preprocessing the input features</b>"""

# X_train

# X_train['bet_type'].unique()

# X_train['sport'].unique()

# X_test['bet_type'].unique()

# X_test['sport'].unique()

encoder = OneHotEncoder()
encoder.fit(X_train.loc[:, ['bet_type', 'sport']])
X_train_new = encoder.transform(X_train.loc[:, ['bet_type', 'sport']]).toarray()
X_test_new = encoder.transform(X_test.loc[:, ['bet_type', 'sport']]).toarray()

# X_train_new

# X_train_new.shape

# X_test_new

# X_test_new.shape

feature_names = encoder.get_feature_names_out()

# feature_names

X_train_new = pd.DataFrame(X_train_new, columns=feature_names)

# X_train_new

X_test_new = pd.DataFrame(X_test_new, columns=feature_names)

# X_test_new

X_train.drop(['bet_type', 'sport'], axis=1, inplace=True)

# X_train

X_test.drop(['bet_type', 'sport'], axis=1, inplace=True)

# X_test

X_train.reset_index(drop=True, inplace=True)

# X_train

X_test.reset_index(drop=True, inplace=True)

# X_test

X_train = pd.concat([X_train, X_train_new], axis=1)

# X_train

# X_train.shape

X_test = pd.concat([X_test, X_test_new], axis=1)

# X_test

# X_test.shape

"""<b>'odds' column</b>"""

# X_train['odds'].mean()

# X_train['odds'].min()

# X_train['odds'].max()

# X_test['odds'].mean()

# X_test['odds'].min()

# X_test['odds'].max()

"""<b>'stake' column</b>"""

# X_train['stake'].mean()

# X_train['stake'].min()

# X_train['stake'].max()

# X_test['stake'].mean()

# X_test['stake'].min()

# X_test['stake'].max()

"""<b>Applying Standard scaling on 'odds' and 'stake' columns</b>"""

scaler = StandardScaler()
scaler.fit(X_train.loc[:, ['odds', 'stake']])
X_train.loc[:, ['odds', 'stake']] = scaler.transform(X_train.loc[:, ['odds', 'stake']])
X_test.loc[:, ['odds', 'stake']] = scaler.transform(X_test.loc[:, ['odds', 'stake']])

# X_train

# X_test

X_train = np.array(X_train)

# X_train

X_test = np.array(X_test)

# X_test

"""<b>Preprocessing output data</b>"""

# y_train

# y_test

encoder = OneHotEncoder()
encoder.fit(y_train.to_numpy().reshape(-1, 1))
y_train_new = encoder.transform(y_train.to_numpy().reshape(-1, 1)).toarray()
y_test_new = encoder.transform(y_test.to_numpy().reshape(-1, 1)).toarray()

# y_train_new

# y_train_new.shape

# y_test_new

# y_test_new.shape

# encoder.get_feature_names_out()

y_train = pd.DataFrame(y_train_new, columns=['Loss', 'Win'])

# y_train

y_test = pd.DataFrame(y_test_new, columns=['Loss', 'Win'])

# y_test

y_train = np.array(y_train)

# y_train

y_test = np.array(y_test)

# y_train

"""<h3><b>Preprocessed data for the model</b></h3>"""

# X_train

# X_train.shape

# y_train

# y_train.shape

# X_test

# X_test.shape

# y_test

# y_test.shape

"""### <b>Training the model</b>"""

n_inputs = len(X_train[0])

# n_inputs

n_outputs = len(y_train[0])

# n_outputs

"""<b>Defining the accuracy calculator function</b>"""

def accuracy(y_pred, y_true):
  if (len(y_true.shape)==1):
    actual = y_true
  elif (len(y_true.shape)==2):
    actual = np.argmax(y_true, axis=1)
  if (len(y_pred.shape)==1):
    predictions = y_pred
  elif (len(y_pred.shape)==2):
    predictions = np.argmax(y_pred, axis=0)
  return accuracy_score(actual, predictions) * 100

"""<b>Initializing the optimizer</b>"""

optimizer = Optimizer_Adam(learning_rate=0.02, decay=1e-5)

"""<b>Initializing the layers</b>"""

# Layer-1
dense1 = Layer_Dense(n_inputs, 64)
activation1 = Activation_ReLU()
dropout1 = Layer_Dropout(0.1)

# Layer-2
dense2 = Layer_Dense(64, n_outputs)
loss_activation = Activation_Softmax_Loss_CategoricalCrossEntropy()

n_epochs = 1000

for epoch in range(n_epochs):
  # Forward pass
  dense1.forward(X_train)
  activation1.forward(dense1.output)
  dropout1.forward(activation1.output)
  dense2.forward(dropout1.output.T)
  net_loss = loss_activation.forward(dense2.output, y_train)
  acc = accuracy(loss_activation.output, y_train)
  if (epoch==0):
    initial_loss = net_loss
    initial_acc = acc
    initial_y_pred = loss_activation.output
  print(f"Epoch-{epoch} - LR = {optimizer.current_learning_rate}, Loss = {net_loss}, Accuracy = {acc}")
  
  # Backward pass
  loss_activation.backward(loss_activation.output, y_train)
  dense2.backward(loss_activation.dinputs)
  dropout1.backward(dense2.dinputs.T)
  activation1.backward(dropout1.dinputs)
  dense1.backward(activation1.dinputs)

  # Updating parameters
  optimizer.pre_update_params()
  optimizer.update_params(dense1)
  optimizer.update_params(dense2)
  optimizer.post_update_params()

print(f"\nInitial Loss = {initial_loss}, Initial Accuracy = {initial_acc}")

def test_model():
  dense1.forward(X_test)
  activation1.forward(dense1.output)
  dense2.forward(activation1.output.T)
  net_loss = loss_activation.forward(dense2.output, y_test)
  acc = accuracy(loss_activation.output, y_test)
  print(f"\nTesting the model:\nLoss = {net_loss}, Accuracy = {acc}")

test_model()

