# -*- coding: utf-8 -*-
"""Neural Network classes.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aIxUwIWf2dV63RTAiI8M9KClMmvVKBTW

## <b>Classes</b>

<b>Important</b> - Batches are same as Samples. Both words may be used interchangeably.

### <b>Dense Layer</b>

The class is designed such that in the input matrix, each <b>row</b> represents one <b>batch of data</b> and each <b>column</b> represents one <b>input feature</b>.
<br><br>
input = [<br>
  [1, 2],<br>
  [3, 4],<br>
  [5, 6]<br>
]
<br><br>
In the above example, there are 3 batches of data, each represented by a row and each batch of data contains 2 input features, each represented by a column.
"""

import numpy as np

class Layer_Dense:
  # Layer Initialization
  def __init__(self, n_inputs, n_neurons, weight_regularizer_l1 = 0, weight_regularizer_l2 = 0, bias_regularizer_l1 = 0, bias_regularizer_l2 = 0):
    # Initializing parameters - weights and biases
    self.weights = np.random.rand(n_neurons, n_inputs)
    # self.biases = np.random.rand(n_neurons, 1)
    self.biases = np.zeros((n_neurons, 1))

    # Initializing regularizers of weights and biases
    self.weight_regularizer_l1 = weight_regularizer_l1
    self.weight_regularizer_l2 = weight_regularizer_l2
    self.bias_regularizer_l1 = bias_regularizer_l1
    self.bias_regularizer_l2 = bias_regularizer_l2

  # Forward pass
  def forward(self, inputs):
    self.inputs = inputs # Remembering the input values
    self.output = np.dot(self.weights, inputs.T) + self.biases # output = z

  # Backward pass
  def backward(self, dvalues): # dvalues = dL/dz
    # Gradients on parameters
    self.dweights = np.dot(dvalues, self.inputs)
    self.dbiases = np.sum(dvalues, axis=1, keepdims=True)

    # Gradients on inputs
    self.dinputs = np.dot(dvalues.T, self.weights)

    # Gradients on regularization
    # L1 Regularization - weights
    if (self.weight_regularizer_l1>0):
      dL1 = np.ones_like(self.weights)
      dL1[self.weights<0] = -1
      self.dweights += self.weight_regularizer_l1 * dL1

    # L2 Regularization - weights
    if (self.weight_regularizer_l2>0):
      self.dweights += 2 * self.weight_regularizer_l2 * self.weights

    # L1 Regularization - biases
    if (self.bias_regularizer_l1>0):
      dL1 = np.ones_like(self.biases)
      dL1[self.biases<0] = -1
      self.dbiases += self.bias_regularizer_l1 * dL1

    # L2 Regularization - biases
    if (self.bias_regularizer_l2>0):
      self.dbiases += 2 * self.bias_regularizer_l2 * self.biases

class Layer_Dropout:
  # Initialize dropout layer
  def __init__(self, rate):
    self.rate = 1 - rate

  # Forward pass
  def forward(self, inputs):
    self.inputs = inputs
    self.binary_mask = np.random.binomial(1, self.rate, size=inputs.shape[0]) / self.rate
    self.output = inputs * self.binary_mask.reshape(-1, 1)

  # Backward pass
  def backward(self, dvalues):
    self.dinputs = dvalues * self.binary_mask.reshape(-1, 1)

"""### <b>ReLU Activation class</b>

ReLU(x) = x if x>0 else 0<br>
ReLU_derivative(x) = 1 if x>0 else 0
"""

class Activation_ReLU:
  # Forward pass
  def forward(self, inputs):
    self.inputs = inputs # Remembering the input values
    self.output = np.maximum(inputs, 0) # output = a

  # Backward pass
  def backward(self, dvalues): # dvalues = dL/da
    self.dinputs = dvalues.copy()
    self.dinputs[self.inputs<=0] = 0

"""### <b>Softmax Activation class</b>"""

class Activation_Softmax:
  # Forward pass
  def forward(self, inputs):
    # normalizing i.e. subtracting maximum z value from each z value in a batch to avoid large exponents
    exp_values = np.exp(inputs - np.max(inputs, axis=0, keepdims=True))

    probabilities = exp_values / np.sum(exp_values, axis=0, keepdims=True)
    self.output = probabilities

"""### <b>Loss class</b>

Parent class for all types of losses
"""

class Loss:
  def calculate(self, output, y_true): # output = y_pred (calculated)
    batch_losses = self.forward(output, y_true) # calls the forward pass function of the child loss class
    data_loss = np.mean(batch_losses) # net data loss is the mean of losses in all batches
    return data_loss

  def regularization_loss(self, layer):
    # 0 by default
    regularization_loss = 0

    # L1 Regularization - weights
    if (layer.weight_regularizer_l1>0):
      regularization_loss += layer.weight_regularizer_l1 * np,sum(np.abs(layer.weights))

    # L2 Regularization - weights
    if (layer.weight_regularizer_l2>0):
      regularization_loss += layer.weight_regularizer_l2 * np.sum(layer.weights * layer.weights)

    # L1 Regularization - biases
    if (layer.bias_regularizer_l1>0):
      regularization_loss += layer.bias_regularizer_l1 * np.sum(np.abs(layer.biases))

    # L2 Regularization - biases
    if (layer.bias_regularizer_l2>0):
      regularization_loss += layer.bias_regularizer_l2 * np.sum(layer.biases * layer.biases)

    return regularization_loss

"""#### <b>Categorical Cross Entropy Loss (for categorical data)</b>

loss = sum(- y_true * ln(y_pred)) - for one batch
"""

class Loss_CategoricalCrossEntropy(Loss):
  # Forward pass
  def forward(self, y_pred, y_true):
    # number of batches of data
    if (len(y_true.shape)==1): # 1D array y_true
      n_batches = len(y_true)
    elif (len(y_true.shape)==2): # 2D array y_true - one-hot encoded
      y_true = y_true.T
      n_batches = len(y_true[0])

    # Clip data to prevent division by 0 in backward pass
    # Clipping - shifting the range of predicted data to a new range
    # Range of each prediction should be between 0 and 1, excluding both.

    y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)
    # Lower limit = 1e-7 = 10^-7 = 0.0000001 (just above 0)
    # Upper limit = 1 - 1e-7 = 1 - 10^-7 = 1 - 0.0000001 = 0.9999999 (just below 1)

    if (len(y_true.shape)==1): # If the actual output matrix consists of label values directly, not one-hot encoded - 1D array
      correct_confidences = y_pred_clipped[y_true, range(n_batches)]

    elif (len(y_true.shape)==2): # If the actual output matrix consists of one-hot encoded labels - 2D array
      correct_confidences = np.sum(y_pred_clipped * y_true, axis=0, keepdims=True)

    # Find negative log (natural) of only those predicted values for whom actual output value is not 0.
    negative_loss_likelihoods = -np.log(correct_confidences)

    return negative_loss_likelihoods

  # Backward pass
  def backward(self, dvalues, y_true):
    # number of batches and output categories
    if (len(y_true.shape)==1): # 1D array y_true
      n_batches = len(y_true)
      n_labels = len(np.unique(y_true))
      y_true = np.eye(n_labels)[y_true].T # one-hot encoding y_true for further operation

    elif (len(y_true.shape)==2): # 2D array y_true - one-hot encoded
      y_true = y_true.T
      n_batches = len(y_true[0])
      n_labels = len(y_true)

    self.dinputs = - y_true / dvalues # calculating gradient
    self.dinputs = self.dinputs / n_batches # normalizing gradient

"""### <b>Combined Softmax Activation and Cross-Entropy Loss class for last layer</b>

dL/dy_pred = - y_true / y_pred but calculating the gradient of softmax output w.r.t. softmax input is complicated.<br>
So, we directly find the derivative of loss w.r.t. softmax input using the direct formula,
<br><br>
<b>dL/dz (or dL/da) = y_pred - y_true</b>
"""

class Activation_Softmax_Loss_CategoricalCrossEntropy:
  # Initialization creates activation and loss objects
  def __init__(self):
    self.activation = Activation_Softmax()
    self.loss = Loss_CategoricalCrossEntropy()

  # Forward pass
  def forward(self, inputs, y_true):
    self.activation.forward(inputs)
    self.output = self.activation.output
    return self.loss.calculate(self.output, y_true)

  # Backward pass
  def backward(self, dvalues, y_true): # dvalues = y_pred here
     # number of batches and output categories
    if (len(y_true.shape)==1): # 1D array y_true
      n_batches = len(y_true)
      n_labels = len(np.unique(y_true))

    elif (len(y_true.shape)==2): # 2D array y_true - one-hot encoded
      y_true = y_true.T
      n_batches = len(y_true[0])
      n_labels = len(y_true)
      y_true = np.argmax(y_true, axis=0) # If y_true is one-hot encoded, convert it back into 1D array of direct output label values for each batch

    self.dinputs = dvalues.copy()
    self.dinputs[y_true, range(n_batches)] -= 1 # calculating gradient
    self.dinputs = self.dinputs / n_batches # normalizing gradient

"""### <b>Linear Activation class</b>

<b>f(x) = x</b>
"""

class Activation_Linear:
  # Forward pass
  def forward(self, inputs):
    self.inputs = inputs
    self.output = inputs

  # Backward pass
  def backward(self, dvalues):
    self.dinputs = dvalues

"""<b>We don't need to explicitly used Linear Activation in a Neural Network as any layer without an activation function performs Linear Activation by default.</b>

### <b>Mean Squared Error Loss class</b>

<b>loss = (y_true - y_pred)^2 / n_batches</b>

<b>d_loss / d_y_pred = 2 * (y_pred - y_true) / n_batches</b>
"""

class Loss_MeanSquaredError(Loss):
  # Forward pass
  def forward(self, y_pred, y_true):
    if (len(y_pred.shape)==2):
      y_pred = y_pred.flatten()
    diff = y_true - y_pred
    squared_diff = diff**2
    return squared_diff

  # Backward pass
  def backward(self, dvalues, y_true):
    if (len(dvalues.shape)==2):
      dvalues = dvalues.flatten()
    n_batches = len(y_true)
    dinputs = 2 * (dvalues - y_true)
    dinputs = dinputs / n_batches
    if (len(dinputs.shape)==1):
      dinputs = dinputs.reshape(1, -1)
    self.dinputs = dinputs

"""### <b>Optimizers</b>

#### <b>Stochastic Gradient Descent (SGD) Optimizer</b>

SGD with learning rate decay and momentum
"""

class Optimizer_SGD:
  # initial learning rate is default to 1 until specified
  # initial decay parameter is default to 0 until specified
  # initial momentum factor is default to 0 until specified
  # initial epoch count will be 0 and will increment after every epoch.
  def __init__(self, learning_rate=1, decay=0, momentum=0):
    self.initial_learning_rate = learning_rate
    self.current_learning_rate = learning_rate
    self.decay = decay
    self.momentum = momentum
    self.epoch = 0 # epoch or iteration number

  # call once before updating parameters in every epoch
  def pre_update_params(self):
    if (self.decay):
      self.current_learning_rate = self.initial_learning_rate / (1 + self.decay * self.epoch)

  # update parameters after every epoch
  def update_params(self, layer):
    # Gradient Descent with momentum
    if (self.momentum):
      # If the layer does not contain momentum arrays, create them and fill with zeroes
      if (not hasattr(layer, 'weight_momentums')):
        layer.weight_momentums = np.zeros_like(layer.weights)
        layer.bias_momentums = np.zeros_like(layer.biases)

      # Build weight updates with momentum
      weight_updates = (-self.current_learning_rate * layer.dweights) + (self.momentum * layer.weight_momentums)
      layer.weight_momentums = weight_updates

      # Build bias updates with momentum
      bias_updates = (-self.current_learning_rate * layer.dbiases) + (self.momentum * layer.bias_momentums)
      layer.bias_momentums = bias_updates

    # Vanilla Gradient Descent
    else:
      weight_updates = -self.current_learning_rate * layer.dweights
      bias_updates = -self.current_learning_rate * layer.dbiases

    # update the parameters
    layer.weights += weight_updates
    layer.biases += bias_updates

  # call once after updating parameters in every epoch
  def post_update_params(self):
    self.epoch += 1

"""#### <b>ADAGRAD Optimizer</b>

<b>Mathematical Procedure for ADAGRAD</b>
- Take the initial learning rate to be <b>n (same for all parameters).</b>
- For each parameter (weight or bias), maintain a history of the gradients of that parameter in previous epochs in the form of <b>cache</b>.
- For first epoch, take cache for each parameter to be 0.
- For each subsequent epoch,<br>
<code>cache = cache + current_param_gradient^2</code><br>
i.e.<br>
<code>cache_w = cache_w + (dL/dw)^2</code><br>
<code>cache_b = cache_b + (dL/db)^2</code>
- After updating the cache, update the params,<br>
<code>w = w - (n / sqrt(cache_w + epsilon)) * dL/dw</code><br>
<code>b = b - (n / sqrt(cache_b + epsilon)) * dL/db</code>
- <b>epsilon</b> is a hyperparamter used to prevent division by 0.
- n is taken along with decay here.
- epsilon value is generally very small (1e-6 or 1e-7)
"""

class Optimizer_Adagrad:
  def __init__(self, learning_rate=1, decay=0, epsilon=1e-7):
    self.initial_learning_rate = learning_rate
    self.current_learning_rate = learning_rate
    self.decay = decay
    self.epoch = 0
    self.epsilon = epsilon

  # call once before updating params in every epoch
  def pre_update_params(self):
    if (self.decay):
      self.current_learning_rate = self.initial_learning_rate / (1 + self.decay * self.epoch)

  def update_params(self, layer):
    # if the layer does not have cache arrays, create them filled with zeroes
    if (not hasattr(layer, 'weight_cache')):
      layer.weight_cache = np.zeros_like(layer.weights)
      layer.bias_cache = np.zeros_like(layer.biases)

    # update cache with squared current gradients
    layer.weight_cache += layer.dweights**2
    layer.bias_cache += layer.dbiases**2

    weight_updates = -(self.current_learning_rate * layer.dweights) / np.sqrt(layer.weight_cache + self.epsilon)
    bias_updates = -(self.current_learning_rate * layer.dbiases) / np.sqrt(layer.bias_cache + self.epsilon)

    layer.weights += weight_updates
    layer.biases += bias_updates

  # call once after updating params in every epoch
  def post_update_params(self):
    self.epoch += 1

"""#### <b>RMSProp Optimizer</b>

<b>Root Mean Square Propagation Optimizer</b>

Similar to ADAGRAD except while accumulating previous gradients in the cache, instead of continually adding gradients to the cache, a weighted sum of past and present gradients are added.

<b>While updating the cache</b>
<br><br>
<code>cache = rho * cache + (1 - rho) * current_parameter_gradient^2</code>
<br><br>
where rho = <b>cache memory decay rate (~0.9)</b>

Keeping rho value around 0.9 means that we are giving much higher weightage to past gradients than the present one, <b>similar to momentum.</b>

RMSProp solves the disadvantage of ADAGRAD as the cache here is not ever increasing at a high rate due to which the update in parameters does not stall.

Updating Parameters formula is same as that in ADAGRAD.
"""

class Optimizer_RMSProp:
  def __init__(self, learning_rate=1, decay=0, epsilon=1e-7, rho=0.9):
    self.initial_learning_rate = learning_rate
    self.current_learning_rate = learning_rate
    self.decay = decay
    self.epoch = 0
    self.epsilon = epsilon
    self.rho = rho

  # call once before updating params in every epoch
  def pre_update_params(self):
    if (self.decay):
      self.current_learning_rate = self.initial_learning_rate / (1 + self.decay * self.epoch)

  def update_params(self, layer):
    # if the layer does not have cache arrays, create them filled with zeroes
    if (not hasattr(layer, 'weight_cache')):
      layer.weight_cache = np.zeros_like(layer.weights)
      layer.bias_cache = np.zeros_like(layer.biases)

    # update cache with the weighted sum of past and present gradients
    layer.weight_cache = self.rho * layer.weight_cache + (1 - self.rho) * layer.dweights**2
    layer.bias_cache = self.rho * layer.bias_cache + (1 - self.rho) * layer.dbiases**2

    weight_updates = -(self.current_learning_rate * layer.dweights) / np.sqrt(layer.weight_cache + self.epsilon)
    bias_updates = -(self.current_learning_rate * layer.dbiases) / np.sqrt(layer.bias_cache + self.epsilon)

    layer.weights += weight_updates
    layer.biases += bias_updates

  # call once after updating params in every epoch
  def post_update_params(self):
    self.epoch += 1

"""#### <b>ADAM Optimizer (used in modern ML models)</b>

<b>Mathematical Procedure</b>
<br><br>
<code>w = w - (n / sqrt(cache_w/(1-beta2^t) + epsilon)) * (momentum_w / (1-beta1^t))</code>
<br><br>
<code>b = b - (n / sqrt(cache_b/(1-beta2^t) + epsilon)) * (momentum_b / (1-beta1^t))</code>
<br><br>
where,<br>
<b>n</b> = current learning rate,<br>
<b>t</b> = number of epochs,<br>
<b>momentum_w</b> = momentum term of w,<br>
<b>momentum_b</b> = momentum term of b,<br>
<b>cache_w</b> = cache term of w,<br>
<b>cache_b</b> = cache term of b,
<br><br>
beta1 and beta2 are parameters of ADAM Optimizer.<br>
<b>beta1</b> - controls momentum term<br>
<b>beta2 (cache memory decay rate)</b> - controls cache term<br>
0 <= beta1 <= 1<br>
0 <= beta2 <= 1
<br><br>
<code>momentum(t) = beta1 * momentum(t-1) + (1 - beta1) * current_gradient</code>
<br><br>
<code>cache(t) = beta2 * cache(t-1) + (1 - beta2) * current_gradient^2</code>
<br><br>
<b>beta1</b> is generally taken to be around <b>0.9</b>.<br>
<b>beta2</b> is generally taken to be around <b>0.999</b>.
<br><br>
learning rate (n) is taken along with decay.
"""

class Optimizer_Adam:
  def __init__(self, learning_rate=0.001, decay=0, epsilon=1e-7, beta_1=0.9, beta_2=0.999):
    self.initial_learning_rate = learning_rate
    self.current_learning_rate = learning_rate
    self.decay = decay
    self.epsilon = epsilon
    self.beta_1 = beta_1
    self.beta_2 = beta_2
    self.epoch = 0

  # call once before updating parameters in every epoch
  def pre_update_params(self):
    if (self.decay):
      self.current_learning_rate = self.initial_learning_rate / (1 + self.decay * self.epoch)

  # update parameters
  def update_params(self, layer):
    # If layer does not contain cache and momentum arrays, create them and fill them with zeroes
    if (not hasattr(layer, 'weight_cache')):
      layer.weight_cache = np.zeros_like(layer.weights)
      layer.weight_momentums = np.zeros_like(layer.weights)
      layer.bias_cache = np.zeros_like(layer.biases)
      layer.bias_momentums = np.zeros_like(layer.biases)

    # update momentums with current gradients
    layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1 - self.beta_1) * layer.dweights
    layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1 - self.beta_1) * layer.dbiases

    # get corrected momentums
    # self.epoch stores the epoch number which starts from 0 so number of epochs completed is always self.epoch + 1
    weight_momentums_corrected = layer.weight_momentums / (1 - self.beta_1**(self.epoch + 1))
    bias_momentums_corrected = layer.bias_momentums / (1 - self.beta_1**(self.epoch + 1))

    # update caches with squared current gradients
    layer.weight_cache = self.beta_2 * layer.weight_cache + (1 - self.beta_2) * layer.dweights**2
    layer.bias_cache = self.beta_2 * layer.bias_cache + (1 - self.beta_2) * layer.dbiases**2

    # get corrected caches
    # self.epoch stores the epoch number which starts from 0 so number of epochs completed is always self.epoch + 1
    weight_cache_corrected = layer.weight_cache / (1 - self.beta_2**(self.epoch + 1))
    bias_cache_corrected = layer.bias_cache / (1 - self.beta_2**(self.epoch + 1))

    # parameter updates
    weight_updates = -(self.current_learning_rate / np.sqrt(weight_cache_corrected + self.epsilon)) * weight_momentums_corrected
    bias_updates = -(self.current_learning_rate / np.sqrt(bias_cache_corrected + self.epsilon)) * bias_momentums_corrected

    # update the parameters
    layer.weights += weight_updates
    layer.biases += bias_updates

  # call once after updating parameters in every epoch
  def post_update_params(self):
    self.epoch += 1