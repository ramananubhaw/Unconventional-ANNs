# -*- coding: utf-8 -*-
"""Training Simple Neural Network on Iris Dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11F0SzqQeDeDnHT8or4QGJWtBNamc1E6J
"""

from neural_network_classes import Layer_Dense, Activation_ReLU, Activation_Softmax_Loss_CategoricalCrossEntropy, Optimizer_SGD

import numpy as np
from sklearn.datasets import load_iris
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split

"""### <b>Dataset + Preprocessing</b>

<b>Using the Iris dataset from scikit-learn</b>
"""

X, y = load_iris(return_X_y=True)

# X

# y

"""<b>Splitting the dataset</b>"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# X_train.shape

# X_test.shape

# y_train.shape

# y_test.shape

"""<b>Scaling the input features</b>"""

scaler = MinMaxScaler()
scaler.fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

# X_train

# y_train

# X_test

# y_test

n_neurons = len(np.unique(y_train))

# n_neurons

n_inputs = len(X_train[0]) # no. of input features in each batch

# n_inputs

"""### <b>Training the model</b>"""

def accuracy(y_pred, y_true):
  predictions = np.argmax(y_pred, axis=0)
  if (len(y_true.shape)==2):
    actual = np.argmax(y_true, axis=0)
  elif (len(y_true.shape)==1):
    actual = y_true
  acc = np.mean(predictions==actual) * 100
  return acc

"""<b>Initializing Optimizer</b>"""

optimizer = Optimizer_SGD(decay=1e-3, momentum=0.5)

"""<b>Initializing all layers and optimizer</b>"""

# Layer-1
dense1 = Layer_Dense(n_inputs, n_neurons)
activation1 = Activation_ReLU()

# Layer-2
dense2 = Layer_Dense(n_neurons, n_neurons)
loss_activation = Activation_Softmax_Loss_CategoricalCrossEntropy()

"""<b>Training the model</b>"""

n_epochs = 250 # no. of iterations of complete forward and backward passes

for epoch in range(n_epochs):
  # Forward pass
  dense1.forward(X_train)
  activation1.forward(dense1.output)
  dense2.forward(activation1.output.T)
  net_loss = loss_activation.forward(dense2.output, y_train)
  acc = accuracy(loss_activation.output, y_train)
  if (epoch==0):
    initial_loss = net_loss
    initial_acc = acc
  print(f"Epoch-{epoch} - LR = {optimizer.current_learning_rate}, Loss = {net_loss}, Accuracy = {acc}")

  # Backward pass
  loss_activation.backward(loss_activation.output, y_train)
  dense2.backward(loss_activation.dinputs)
  # dense2.weights -= learning_rate * dense2.dweights
  # dense2.biases -= learning_rate * dense2.dbiases
  activation1.backward(dense2.dinputs.T)
  dense1.backward(activation1.dinputs)
  # dense1.weights -= learning_rate * dense1.dweights
  # dense1.biases -= learning_rate * dense1.dbiases

  # Updating parameters
  optimizer.pre_update_params()
  optimizer.update_params(dense2)
  optimizer.update_params(dense1)
  optimizer.post_update_params()

"""<b>Testing the model after training</b>"""

print(f"\nInitial Loss = {initial_loss}\nInitial Accuracy = {initial_acc}")

def test_model():
  print("\nTesting the model:")
  dense1.forward(X_test)
  activation1.forward(dense1.output)
  dense2.forward(activation1.output.T)
  net_loss = loss_activation.forward(dense2.output, y_test)
  acc = accuracy(loss_activation.output, y_test)
  print(f"Loss = {net_loss}, Accuracy = {acc}")

test_model()

