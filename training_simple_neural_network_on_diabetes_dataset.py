# -*- coding: utf-8 -*-
"""Training Simple Neural Network on Diabetes dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KQ8UXeE21hfV1ukUcBYnd4NMby2pnWni
"""

from neural_network_classes import Layer_Dense, Activation_ReLU, Loss_MeanSquaredError, Optimizer_SGD

import numpy as np
from sklearn.datasets import load_diabetes
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

"""### <b>Dataset + Preprocessing</b>"""

X, y = load_diabetes(return_X_y=True, as_frame=True)

# X

# y

# X.isnull().sum()

# X.mean()

# X.max()

# X.min()

# y.isnull().sum()

# y.min()

# y.max()

# y.mean()

"""<b>Splitting into training and test data</b>"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# X_train.shape

# y_train.shape

# X_test.shape

# y_test.shape

"""<b>Noting the mean and standard deviation of the original y values before scaling</b>"""

mean = y_train.mean()

# mean

sd = y_train.var()**(1/2)

# sd

"""<b>Scaling output y</b>"""

scaler = StandardScaler()
scaler.fit(y_train.to_numpy().reshape(-1, 1))
y_train_new = scaler.transform(y_train.to_numpy().reshape(-1, 1))
y_test_new = scaler.transform(y_test.to_numpy().reshape(-1, 1))

# y_train_new

# y_test_new

"""<b>Preparing training data and test data according the format required by the model</b>"""

X_train = np.array(X_train)

# X_train

# X_train.shape

X_test = np.array(X_test)

# X_test

# X_test.shape

y_train = y_train_new.T.flatten()

# y_train = y_train.to_numpy()

# y_train

# y_train.shape

y_test = y_test_new.T.flatten()

# y_test = y_test.to_numpy()

# y_test

# y_test.shape

# """### <b>Performing single forward and backward pass</b>"""

# n_inputs = len(X_train[0])

# n_inputs

# n_neurons = n_inputs

# n_neurons

# """<b>Initializing the layers</b>"""

# dense1 = Layer_Dense(n_inputs, n_neurons)

# """First layer will perform Linear Activation so no need to explicitly create Activation object for this layer."""

# dense2 = Layer_Dense(n_inputs, n_neurons)

# """Second layer will perform <b>ReLU Activation</b> on its output."""

# activation2 = Activation_ReLU()

# """Last Layer will be the output layer with only <b>one neuron</b> with Linear Activation"""

# dense3 = Layer_Dense(n_inputs, 1)

# loss_mse = Loss_MeanSquaredError()

# """Optimizer

# Taking learning rate to be 0.001 here.
# """

# learning_rate = 0.001

# optimizer = Optimizer_SGD(learning_rate)

# """#### <b>Forward Pass</b>"""

# dense1.forward(X_train)

# dense1.output

# dense1.output.shape

# dense1.output.T.shape

# """So, we will pass the inverse of this output matrix of dense1 to dense2 to match the input matrix format of Layer_Dense class."""

# dense2.forward(dense1.output.T)

# dense2.output

# dense2.output.shape

# activation2.forward(dense2.output)

# activation2.output

# activation2.output.shape

# activation2.output.T.shape

# """So, we will pass the inverse of this output matrix of activation2 to dense3 to match the input matrix format of Layer_Dense class."""

# dense3.forward(activation2.output.T)

# dense3.output # y_pred

# y_train.shape

# dense3.output.shape

# """We do not need to flatten the y_pred matrix i.e. the output of dense3 layer as that will be handled by loss_mse internally."""

# net_loss = loss_mse.calculate(dense3.output, y_train)

# net_loss

# """<b>We need to minimize this loss. The closer this loss value gets to 0, the more accurate our model becomes.</b>

# #### <b>Backward Pass</b>
# """

# loss_mse.backward(dense3.output, y_train)

# loss_mse.dinputs

# loss_mse.dinputs.shape

# dense3.backward(loss_mse.dinputs)

# dense3.dinputs

# dense3.dinputs.shape

# dense3.dweights

# dense3.dweights.shape

# dense3.dbiases

# dense3.dbiases.shape

# dense3.dinputs.T.shape

# dense3.weights

# dense3.biases

# optimizer.update_params(dense3)

# dense3.weights

# dense3.biases

# """So, we will pass the transpose of this dinputs matrix of dense3 to activation2 layer for backward pass to match the required 'dvalues' matrix shape criteria of Activation_ReLU class."""

# activation2.backward(dense3.dinputs.T)

# activation2.dinputs

# activation2.dinputs.shape

# dense2.backward(activation2.dinputs)

# dense2.dinputs

# dense2.dinputs.shape

# dense2.dweights

# dense2.dweights.shape

# dense2.dbiases

# dense2.dbiases.shape

# dense2.weights

# dense2.biases

# optimizer.update_params(dense2)

# dense2.weights

# dense2.biases

# dense2.dinputs.shape

# dense2.dinputs.T.shape

# """So, we will pass the transpose of this matrix to the dense1 layer for backward pass."""

# dense1.backward(dense2.dinputs.T)

# dense1.dinputs

# dense1.dinputs.shape

# dense1.dweights.shape

# optimizer.update_params(dense1)

# dense1.weights

# dense1.biases

"""### <b>Training the model</b>"""

n_inputs = len(X_train[0])

# n_inputs

n_neurons = n_inputs

# n_neurons

"""<b>Initializing the optimizer</b>"""

optimizer = Optimizer_SGD(learning_rate=0.01, decay=1e-3, momentum=0.1)

"""<b>Initializing the layers</b>

First layer will perform Linear Activation so no need to explicitly create Activation object for this layer.

Second layer will perform <b>ReLU Activation</b> on its output.

Last Layer will be the output layer with only <b>one neuron</b> with Linear Activation

Mean Squared Error (MSE) loss function will be used here.
"""

dense1 = Layer_Dense(n_inputs, n_neurons)
dense2 = Layer_Dense(n_inputs, n_neurons)
activation2 = Activation_ReLU()
dense3 = Layer_Dense(n_inputs, 1)
loss_mse = Loss_MeanSquaredError()

n_epochs = 10000

for epoch in range(n_epochs):
  # Forward pass
  dense1.forward(X_train)
  dense2.forward(dense1.output.T)
  activation2.forward(dense2.output)
  dense3.forward(activation2.output.T)
  net_loss = loss_mse.calculate(dense3.output, y_train)
  if (epoch==0):
    initial_loss = net_loss
    initial_y_pred = dense3.output
  print(f"Epoch-{epoch} - LR = {optimizer.current_learning_rate}, Loss = {net_loss}")

  # Backward pass
  loss_mse.backward(dense3.output, y_train)
  dense3.backward(loss_mse.dinputs)
  # dense3.weights -= learning_rate * dense3.dweights
  # dense3.biases -= learning_rate * dense3.dbiases
  activation2.backward(dense3.dinputs.T)
  dense2.backward(activation2.dinputs)
  # dense2.weights -= learning_rate * dense2.dweights
  # dense2.biases -= learning_rate * dense2.dbiases
  dense1.backward(dense2.dinputs.T)
  # dense1.weights -= learning_rate * dense1.dweights
  # dense1.biases -= learning_rate * dense1.dbiases

  # Updating parameters
  optimizer.pre_update_params()
  optimizer.update_params(dense3)
  optimizer.update_params(dense2)
  optimizer.update_params(dense1)
  optimizer.post_update_params()

print(f"\nInitial Loss = {initial_loss}")

def test_model():
  print("\nTesting the model:")
  dense1.forward(X_test)
  dense2.forward(dense1.output.T)
  activation2.forward(dense2.output)
  dense3.forward(activation2.output.T)
  net_loss = loss_mse.calculate(dense3.output, y_test)
  print(f"Loss = {net_loss}")

test_model()

# initial_y_pred

# dense3.output

# y_test

