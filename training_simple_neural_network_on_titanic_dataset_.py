# -*- coding: utf-8 -*-
"""Training Simple Neural Network on Titanic Dataset .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1A6Ro8f-WCzXugwEIXZ06kwl7kE64Bt6u
"""

from neural_network_classes import Layer_Dense, Activation_ReLU, Activation_Softmax_Loss_CategoricalCrossEntropy, Optimizer_SGD

import numpy as np
import pandas as pd
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score

"""### <b>Dataset + Preprocessing</b>"""

df = pd.read_csv('Titanic-Dataset.csv')

# df

# df['Cabin'].unique()

df.drop(labels=['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1, inplace=True)

# df

# df.isnull().sum()

# df[df['Embarked'].isnull()]

df.drop(df[df['Embarked'].isnull()].index, axis=0, inplace=True)

# df.isnull().sum() / len(df)

X = df.iloc[:, :-1]
y = df.iloc[:, -1]

# X

# y

# y.unique()

# y.isnull().sum()

"""<b>Splitting the data into training and test sets</b>"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# X_train.shape

# y_train.shape

# X_test.shape

# y_test.shape

"""<b>Dealing with the 'Sex' column</b>"""

# X_train['Sex'].unique()

# X_test['Sex'].unique()

# X_train['Sex'].to_numpy()

encoder = OneHotEncoder()
encoder.fit(X_train['Sex'].to_numpy().reshape(-1, 1))
new_sex_train = encoder.transform(X_train['Sex'].to_numpy().reshape(-1, 1))
new_sex_test = encoder.transform(X_test['Sex'].to_numpy().reshape(-1, 1))

# new_sex_train.toarray()

# new_sex_test.toarray()

# encoder.get_feature_names_out()

new_sex_train = pd.DataFrame(new_sex_train.toarray(), columns=['Female', 'Male'])

# new_sex_train

# X_train['Sex']

new_sex_test = pd.DataFrame(new_sex_test.toarray(), columns=['Female', 'Male'])

# new_sex_test

# X_test['Sex']

X_train.drop('Sex', axis=1, inplace=True)

# X_train

X_test.drop('Sex', axis=1, inplace=True)

# X_test

X_train.reset_index(drop=True, inplace=True)

# X_train

X_test.reset_index(drop=True, inplace=True)

# X_test

X_train = pd.concat([X_train, new_sex_train], axis=1)

X_test = pd.concat([X_test, new_sex_test], axis=1)

# X_train

# X_test

"""<b>Dealing with the missing values in 'Age' column</b>"""

# X_train.isnull().sum() / len(X_train)

# X_test.isnull().sum() / len(X_test)

imputer = SimpleImputer()
imputer.fit(X_train['Age'].to_numpy().reshape(-1, 1))
X_train['Age'] = imputer.transform(X_train['Age'].to_numpy().reshape(-1, 1))
X_test['Age'] = imputer.transform(X_test['Age'].to_numpy().reshape(-1, 1))

# X_train

# X_train.isnull().sum()

# X_test

# X_test.isnull().sum()

# X_train.nunique()

"""<b>Age and Fare columns are numerical with different range of values so we scale them down to avoid any bias in the model.</b>"""

# X_train.loc[:, ['Age', 'Fare']]

scaler = StandardScaler()
scaler.fit(X_train.loc[:, ['Age', 'Fare']])
age_fare_train = scaler.transform(X_train.loc[:, ['Age', 'Fare']])
age_fare_test = scaler.transform(X_test.loc[:, ['Age', 'Fare']])

# age_fare_train

# age_fare_test

# scaler.get_feature_names_out()

age_fare_train = pd.DataFrame(age_fare_train, columns=scaler.get_feature_names_out())

# age_fare_train

age_fare_test = pd.DataFrame(age_fare_test, columns=scaler.get_feature_names_out())

# age_fare_test

X_train.drop(['Age', 'Fare'], axis=1, inplace=True)

# X_train

X_test.drop(['Age', 'Fare'], axis=1, inplace=True)

# X_test

X_train = pd.concat([X_train, age_fare_train], axis=1)

X_test = pd.concat([X_test, age_fare_test], axis=1)

# X_train

# X_test

"""<b>Training and Test input features are preprocessed</b>

<b>Now encoding the output categories of train and test sets</b>
"""

# y_train

# y_test

encoder = OneHotEncoder()
encoder.fit(y_train.to_numpy().reshape(-1, 1))
y_train_new = encoder.transform(y_train.to_numpy().reshape(-1, 1)).toarray()
y_test_new = encoder.transform(y_test.to_numpy().reshape(-1, 1)).toarray()

# y_train_new

# y_test_new

# encoder.get_feature_names_out()

y_train_new = pd.DataFrame(y_train_new, columns=['C', 'Q', 'S'])

y_test_new = pd.DataFrame(y_test_new, columns=['C', 'Q', 'S'])

# y_train_new

# y_train

# y_test_new

y_train = y_train_new

y_test = y_test_new

"""<b>So, the train and test set output catgories are also encoded.</b>

<b>Now, we will convert train and test data into required array formats to pass to the model.</b>
"""

X_train = np.array(X_train)

# X_train

X_test = np.array(X_test)

# X_test

y_train = np.array(y_train)

# y_train

y_test = np.array(y_test)

# y_test

# X_train.shape

# y_train.shape

# X_test.shape

# y_test.shape

"""### <b>Training the model</b>"""

n_neurons = len(y_train[0])

# n_neurons

n_inputs = len(X_train[0]) # no. of input features in each batch

# n_inputs

def accuracy(y_pred, y_true):
  if (len(y_pred.shape)==2):
    predictions = np.argmax(y_pred, axis=0)
  elif (len(y_pred.shape)==1):
    predictions = y_pred
  if (len(y_true.shape)==2):
    y_true = y_true.T
    actual = np.argmax(y_true, axis=0)
  elif (len(y_true.shape)==1):
    actual = y_true
  # acc = np.mean(predictions==actual) * 100
  return accuracy_score(actual, predictions)

"""<b>Initializing Optimizer</b>"""

optimizer = Optimizer_SGD(decay=1e-3)

"""<b>Initializing all layers</b>"""

# Layer-1
dense1 = Layer_Dense(n_inputs, n_neurons)
activation1 = Activation_ReLU()

# Layer-2
dense2 = Layer_Dense(n_neurons, n_neurons)
loss_activation = Activation_Softmax_Loss_CategoricalCrossEntropy()

"""<b>Training the model</b>"""

n_epochs = 2000 # no. of iterations of complete forward and backward passes

for epoch in range(n_epochs):
  # Forward pass
  dense1.forward(X_train)
  activation1.forward(dense1.output)
  dense2.forward(activation1.output.T)
  net_loss = loss_activation.forward(dense2.output, y_train)
  acc = accuracy(loss_activation.output, y_train)
  if (epoch==0):
    initial_loss = net_loss
    initial_acc = acc
  print(f"Epoch-{epoch} - LR = {optimizer.current_learning_rate}, Loss = {net_loss}, Accuracy = {acc}")

  # Backward pass
  loss_activation.backward(loss_activation.output, y_train)
  dense2.backward(loss_activation.dinputs)
  # dense2.weights -= learning_rate * dense2.dweights
  # dense2.biases -= learning_rate * dense2.dbiases
  activation1.backward(dense2.dinputs.T)
  dense1.backward(activation1.dinputs)
  # dense1.weights -= learning_rate * dense1.dweights
  # dense1.biases -= learning_rate * dense1.dbiases

  # Updating parameters
  optimizer.pre_update_params()
  optimizer.update_params(dense2)
  optimizer.update_params(dense1)
  optimizer.post_update_params()

"""<b>Testing the model after training</b>"""

print(f"\nInitial Loss = {initial_loss}\nInitial Accuracy = {initial_acc}")

def test_model():
  print("\nTesting the model:")
  dense1.forward(X_test)
  activation1.forward(dense1.output)
  dense2.forward(activation1.output.T)
  net_loss = loss_activation.forward(dense2.output, y_test)
  acc = accuracy(loss_activation.output, y_test)
  print(f"Loss = {net_loss}, Accuracy = {acc}")

test_model()

# y_train_new = np.argmax(y_train, axis=1)

# """<b>Comparing the accuracy of this model with that of a Logistic Regression model</b>"""

# from sklearn.linear_model import LogisticRegression
# clf = LogisticRegression()
# clf.fit(X_train, y_train_new)

# y_pred = clf.predict(X_test)

# # y_pred

# y_test_new = np.argmax(y_test, axis=1)

# from sklearn.metrics import accuracy_score
# print(accuracy_score(y_test_new, y_pred))

# # accuracy(y_pred, y_test_new)