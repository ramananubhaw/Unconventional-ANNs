# -*- coding: utf-8 -*-
"""Training Simple Neural Network on Spiral Data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AZiP0izm-5XBWmvg4qPaQa1T3Yy2kRif

### <b>Dataset + Preprocessing</b>
"""

from neural_network_classes import Layer_Dense, Layer_Dropout, Activation_ReLU, Activation_Softmax_Loss_CategoricalCrossEntropy, Optimizer_Adam

# pip install nnfs

# from nnfs.datasets import spiral_data
# import nnfs
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from IPython.display import clear_output

# nnfs.init()

"""Taking dataset with 3000 points and 3 classes - R, G and B with 1000 points belonging to each class."""

df = pd.read_csv('spiral_data.csv')

# df

X = np.array(df.iloc[:, :-1])
y = np.array(df.iloc[:, -1])

# X, y = spiral_data(samples=1000, classes=3)

# X

# X.shape

# y

# y.shape

# plt.scatter(X[:,0], X[:,1], c=y, cmap='brg', s=6)
# plt.show()

"""Since the color map applied above is <b>brg</b>, it means that,<br>
<b>y = 0</b> means <b>Blue</b><br>
<b>y = 1</b> means <b>Red</b><br>
<b>y = 2</b> means <b>Green</b>

<b>Splitting into training and test data</b>
"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# X_train.shape

# y_train.shape

# X_test.shape

# y_test.shape

"""<b>There are no missing values in this dataset as it has been generated by us.</b>"""

# X_train.mean()

# X_test.mean()

# np.unique(y_train)

# np.unique(y_test)

"""<b>So, the dataset is already preprocessed.</b>

### <b>Training the model</b>
"""

"""<b>Plotting function</b>"""

def plot_decision_boundary(ax1, ax2, X, y, y_pred, accuracy, epoch):
  # Clearing previous plot
  ax1.clear()
  ax2.clear()

  preds = np.argmax(y_pred, axis=0)
  
  # Plotting
  ax1.scatter(X[:, 0], X[:, 1], c=y, cmap='brg', s=6)
  ax1.set_title('Actual')
  ax2.scatter(X[:, 0], X[:, 1], c=preds, cmap='brg', s=6)
  ax2.set_title('Predicted')
  fig.suptitle(f"Epoch-{epoch} - Accuracy = {accuracy}")

"""<b>Defining the accuracy function</b>"""

def accuracy(y_pred, y_true):
  predictions = np.argmax(y_pred, axis=0)
  if (len(y_true.shape)==2):
    actual = np.argmax(y_true, axis=0)
  elif (len(y_true.shape)==1):
    actual = y_true
  acc = np.mean(predictions==actual) * 100
  return acc

"""<b>Initializing the optimizer</b>"""

optimizer = Optimizer_Adam(learning_rate=0.02, decay=1e-5)

"""<b>Initializing all layers</b>"""

# Layer-1
dense1 = Layer_Dense(2, 64, weight_regularizer_l2=5e-4, bias_regularizer_l2=5e-4)
# dense1 = Layer_Dense(2, 64)
activation1 = Activation_ReLU()
dropout1 = Layer_Dropout(0.1)

# Layer-2
dense2 = Layer_Dense(64, 3)
loss_activation = Activation_Softmax_Loss_CategoricalCrossEntropy()

"""<b>Training the model</b>"""

n_epochs = 5000 # no. of iterations of complete forward and backward passes

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))  # 1 row, 2 columns
plt.ion()
plt.show()

for epoch in range(n_epochs):
  # Forward pass
  dense1.forward(X_train)
  activation1.forward(dense1.output)
  dropout1.forward(activation1.output)
  dense2.forward(dropout1.output.T)
  data_loss = loss_activation.forward(dense2.output, y_train)
  regularization_loss = (
    loss_activation.loss.regularization_loss(dense1) + loss_activation.loss.regularization_loss(dense2)
  )
  net_loss = data_loss + regularization_loss
  acc = accuracy(loss_activation.output, y_train)
  if (epoch==0):
    initial_loss = net_loss
    initial_acc = acc
  print(f"Epoch-{epoch} - LR = {optimizer.current_learning_rate}, Net Loss = {net_loss}, Data Loss = {data_loss}, Reg Loss = {regularization_loss},  Accuracy = {acc}")
  
  # Update plot
  # if (epoch%100==0):
  plot_decision_boundary(ax1, ax2, X_train, y_train, loss_activation.output, acc, epoch)
  plt.pause(0.01)

  # Backward pass
  loss_activation.backward(loss_activation.output, y_train)
  dense2.backward(loss_activation.dinputs)
  # dense2.weights -= learning_rate * dense2.dweights
  # dense2.biases -= learning_rate * dense2.dbiases
  dropout1.backward(dense2.dinputs.T)
  activation1.backward(dropout1.dinputs)
  dense1.backward(activation1.dinputs)
  # dense1.weights -= learning_rate * dense1.dweights
  # dense1.biases -= learning_rate * dense1.dbiases

  # Updating the parameters
  optimizer.pre_update_params()
  optimizer.update_params(dense2)
  optimizer.update_params(dense1)
  optimizer.post_update_params()

plot_decision_boundary(ax1, ax2, X_train, y_train, loss_activation.output, acc, n_epochs-1)
plt.ioff()
plt.show()

"""### <b>Testing the model</b>"""

print(f"\nInitial Loss = {initial_loss}\nInitial Accuracy = {initial_acc}")

def test_model():
  dense1.forward(X_test)
  activation1.forward(dense1.output)
  dense2.forward(activation1.output.T)
  net_loss = loss_activation.forward(dense2.output, y_test)
  acc = accuracy(loss_activation.output, y_test)
  print(f"\nTesting the model\nLoss = {net_loss}, Accuracy = {acc}")

test_model()